I have the attached code that takes binary data files from one source and puts it in a database. I have another data source that my raw data is in csv format that I need to load into databases with the same format as the script below. I am reading all of these csv's from one directory into a separate database per intersection. 

import os
import glob
import json
import zlib
import struct
import sqlite3
import pytz
from datetime import datetime, timedelta, timezone

# --- Configuration ---
DEFAULT_STATE_FILE = 'processing_state.json'
DEFAULT_DB_FILE = 'spm_data.db'
INTERVAL_MINUTES = 1

# --- 1. Helper Functions ---

def get_timestamp_from_filename(filename):
    """
    Parses 'ECON_10.70.10.51_2025_11_25_2136.datZ' -> datetime object (Naive)
    """
    try:
        base = os.path.basename(filename)
        name_part = base.replace('.datZ', '')
        parts = name_part.split('_')
        # Expecting last 4 parts: YYYY, MM, DD, HHMM
        date_str = "_".join(parts[-4:]) 
        return datetime.strptime(date_str, "%Y_%m_%d_%H%M")
    except Exception:
        return None

def load_state(state_path):
    """
    Loads the last processed timestamp from JSON.
    Returns a naive datetime object (matching filename format).
    """
    if os.path.exists(state_path):
        try:
            with open(state_path, 'r') as f:
                data = json.load(f)
                ts_str = data.get('last_processed_time')
                if ts_str:
                    return datetime.fromisoformat(ts_str)
        except Exception as e:
            print(f"Warning: Could not read state file: {e}")
    return None

def save_state(state_path, last_time):
    """Updates the state file."""
    try:
        with open(state_path, 'w') as f:
            # We save as ISO format string
            json.dump({'last_processed_time': last_time.isoformat()}, f)
    except Exception as e:
        print(f"Warning: Could not save state file: {e}")

def to_utc_epoch(dt_obj):
    """
    Uses pytz to handle MST/MDT transitions automatically.
    """
    # 1. Define the timezone
    mst_tz = pytz.timezone('US/Mountain')
    
    # 2. Localize the naive time
    # This attaches the timezone and checks if it falls in DST or Standard time
    dt_aware = mst_tz.localize(dt_obj)
    
    # 3. Convert to UTC timestamp
    return dt_aware.timestamp()

def init_db(db_path):
    """Initialize SQLite DB with REAL (Float) type for speed/compactness."""
    conn = sqlite3.connect(db_path)
    c = conn.cursor()
    c.execute('''
        CREATE TABLE IF NOT EXISTS logs (
            timestamp REAL, 
            event_type INTEGER,
            parameter INTEGER
        )
    ''')
    # Index for fast queries
    c.execute('CREATE INDEX IF NOT EXISTS idx_ts ON logs (timestamp)')
    conn.commit()
    return conn

# --- 2. Core Decoding Logic ---

def decode_single_file(filepath, base_time):
    """
    Decodes .datZ file into a list of tuples (epoch_float, event, param).
    """
    records = []
    
    # Corruption Guard: Skip ballooned files
    if os.path.getsize(filepath) > 50000:
        print(f"Skipping corrupt file (Size > 50KB): {filepath}")
        return []

    try:
        with open(filepath, 'rb') as f:
            content = zlib.decompress(f.read())
    except Exception as e:
        print(f"Error decompressing {filepath}: {e}")
        return []

    # Find Binary Payload
    marker = b"Phases in use:"
    marker_pos = content.find(marker)
    if marker_pos == -1: return [] 
    
    newline_pos = content.find(b'\n', marker_pos)
    binary_bytes = content[newline_pos+1:]

    # Parse Rows
    row_size = 4
    num_rows = len(binary_bytes) // row_size
    
    for i in range(num_rows):
        chunk = binary_bytes[i*row_size : (i+1)*row_size]
        try:
            # >BBH: Big-Endian, UChar, UChar, UShort
            evt, param, offset = struct.unpack('>BBH', chunk)
            
            # 1. Calculate Time (Naive)
            row_time = base_time + timedelta(seconds=offset/10.0)
            
            # 2. Force UTC & Convert to Float
            ts_val = to_utc_epoch(row_time)
            
            records.append((ts_val, evt, param))
        except struct.error:
            continue
            
    return records

# --- 3. Batch Processor ---

def process_datz_batch(input_dir, output_db=DEFAULT_DB_FILE, state_file=DEFAULT_STATE_FILE):
    """
    Reads .datZ files, checks continuity, and appends Epoch Floats to SQLite.
    """
    last_processed_time = load_state(state_file)
    if last_processed_time:
        print(f"Resuming from: {last_processed_time}")

    # Scan and Sort Files
    files = glob.glob(os.path.join(input_dir, "*.datZ"))
    file_map = []
    for f in files:
        ts = get_timestamp_from_filename(f)
        if ts:
            # Filter: Only process new files
            if last_processed_time is None or ts > last_processed_time:
                file_map.append((ts, f))
    
    file_map.sort(key=lambda x: x[0])
    
    if not file_map:
        print("No new files to process.")
        return

    # Connect to DB
    conn = init_db(output_db)
    cursor = conn.cursor()

    new_records = []
    current_batch_last_time = last_processed_time
    
    print(f"Processing {len(file_map)} new files...")

    for file_ts, filepath in file_map:
        
        # --- Continuity Logic (Gap Detection) ---
        if current_batch_last_time is not None:
            expected_next = current_batch_last_time + timedelta(minutes=INTERVAL_MINUTES)
            
            if file_ts > expected_next:
                print(f"  -> Gap detected: {current_batch_last_time} -> {file_ts}")
                
                # Insert Break Record (-1, -1)
                # Use the START of the new file's minute as the timestamp
                break_ts_val = to_utc_epoch(file_ts)
                new_records.append((break_ts_val, -1, -1))
        
        # --- Decode File ---
        file_rows = decode_single_file(filepath, file_ts)
        new_records.extend(file_rows)
        
        # Update tracker (Naive time kept for filename comparisons)
        current_batch_last_time = file_ts

    # --- Bulk Insert ---
    if new_records:
        try:
            cursor.executemany('INSERT INTO logs VALUES (?,?,?)', new_records)
            conn.commit()
            print(f"Committed {len(new_records)} rows to {output_db}")
        except sqlite3.Error as e:
            print(f"Database Error: {e}")
            conn.close()
            return
    else:
        print("Processed files, but no binary events found (empty or corrupt).")

    conn.close()

    # --- Update State ---
    # We save the naive 'current_batch_last_time' because filenames are naive
    save_state(state_file, current_batch_last_time)
    print(f"State updated to: {current_batch_last_time}")

if __name__ == "__main__":
    # Example usage
    # Ensure 'data' folder exists or change path
    WD = 'C:\\Users\\rhansen\\Documents\\Python\\SPMs\\Intersections\\Franklin & Ward\\Data'
    DATZ_PATH = 'G:\\Python\\SPM_Data_Archive\\Intersections\\Franklin & Ward\\Data\\DATZ\\Archive'
    #DATZ_PATH = './datz'
    DB_PATH = 'spm_data.db'
    os.chdir(WD)
    if os.path.exists(DATZ_PATH):
        process_datz_batch(DATZ_PATH, DB_PATH)
    else:
        print("Please set input directory path.")