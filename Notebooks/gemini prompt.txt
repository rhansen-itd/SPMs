I have code in process_datz_db.py that takes binary data files from one data source and puts it in a database. I have another data source from which the raw data is in csv format that I need to load into databases with the same format as the output from the process_datz_db workflow. I am reading all of these csv's from the other datasource from one directory into a separate database per intersection. Within that directory, there are multiple CSV files with names [intersection number, 3 digit integer]_Events_[timestamp].csv. There is a sample 201_Events...csv in the attached repository. Here is the basics of what the new module (which will eventually merge with/replace as applicable the process_achd.py module) will need to do:

 - Determine which intersections are in the directory.
 - For each intersection loop through each csv file and read start and end times (available from lines 2 and 3 of the file). Remove any csv from the list for further processing if the dates are fully contained within the data range of the other files (e.g. if the file is data from June 12 2024 but there is another file that is June 1- June 15 2024).
   - Using those start and end times read from each csv of that intersection, determine data continuity to be prepared to notate it the same way as the process_datz_db file does (i.e. prepare a list of start times data that is discontinuous with previous data. Based on how this data is retrieved, the end time of a full day is 23:59:59 so the next day 00:00:00 is continuous data). Create a well-structured json file for each intersection with the date ranges present.
   - Read each csv into a dataframe. At the timestamps where there is a discontinuity, insert a (-1,-1) row consistent with the process_datz_db module. Drop the "Event Description" column. Rename columns consistent with columns in process_datz_db (Time, Code, and ID).
   - Drop any duplicate rows (i.e. if there are overlapping dates and therefore duplicated entries).
   - Save to database titled [intersectionnumber]_data.db
