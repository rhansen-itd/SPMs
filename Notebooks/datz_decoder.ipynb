{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9da1b264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Data ---\n",
      "Fetching DB data from 2025-11-25 17:00:00-07:00 to 2025-11-28 22:00:00-07:00...\n",
      "Reading 4 pickle file(s)...\n",
      "\n",
      "--- Comparison Results ---\n",
      "Time Window: 2025-11-25 17:00:00 to 2025-11-28 22:00:00\n",
      "New DB Rows:   1,097,789\n",
      "Old Pickle Rows: 1,097,789\n",
      "\n",
      "New DB Analysis:\n",
      "  - Real Data Rows: 1,080,902\n",
      "  - Generated Breaks (Code 0): 16887\n",
      "\n",
      "⚠️ MISMATCH: Count difference of -16887 rows.\n",
      "\n",
      "--- Visual Sample (First 5 Rows) ---\n",
      "NEW (DB):\n",
      "             TS_start  Code  ID\n",
      "0 2025-11-25 17:00:00     3   7\n",
      "1 2025-11-25 17:00:00    44   4\n",
      "2 2025-11-25 17:00:00    81  37\n",
      "3 2025-11-25 17:00:00    81  38\n",
      "4 2025-11-25 17:00:00    81  42\n",
      "OLD (Pickle):\n",
      "                 TS_start  Code  ID\n",
      "0 2025-11-25 17:00:00.200     3   7\n",
      "1 2025-11-25 17:00:00.200    44   4\n",
      "2 2025-11-25 17:00:00.200    81  37\n",
      "3 2025-11-25 17:00:00.200    81  38\n",
      "4 2025-11-25 17:00:00.200    81  42\n",
      "\n",
      "--- Visual Sample (Last 5 Rows) ---\n",
      "NEW (DB):\n",
      "                             TS_start  Code  ID\n",
      "1097784 2025-11-28 21:59:57.200000048    10   8\n",
      "1097785 2025-11-28 21:59:57.200000048    64   2\n",
      "1097786 2025-11-28 21:59:57.200000048    64   4\n",
      "1097787 2025-11-28 21:59:57.799999952    82  37\n",
      "1097788 2025-11-28 21:59:58.200000048    81  37\n",
      "OLD (Pickle):\n",
      "                       TS_start  Code  ID\n",
      "1097784 2025-11-28 21:59:58.200    10   8\n",
      "1097785 2025-11-28 21:59:58.200    64   2\n",
      "1097786 2025-11-28 21:59:58.200    64   4\n",
      "1097787 2025-11-28 21:59:58.800    82  37\n",
      "1097788 2025-11-28 21:59:59.200    81  37\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import pytz\n",
    "import glob\n",
    "\n",
    "# --- Configuration ---\n",
    "# Paths as specified in your prompt\n",
    "DB_PATH = Path('../intersections/franklin & smeed/data/datz/spm_data.db')\n",
    "PICKLE_DIR = Path('../intersections/franklin & smeed/data/DataFrames')\n",
    "\n",
    "# Timezone settings\n",
    "LOCAL_TZ = pytz.timezone('US/Mountain')\n",
    "\n",
    "def get_db_data(start_dt, end_dt):\n",
    "    \"\"\"\n",
    "    Reads from SQLite, converts Epoch -> Datetime, and localizes to MST.\n",
    "    \"\"\"\n",
    "    if not DB_PATH.exists():\n",
    "        print(f\"❌ Database not found: {DB_PATH}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 1. Convert Input (Local Naive) -> UTC Epoch for querying\n",
    "    # Assume input string is \"Wall Clock Time\" in Garden City (MST)\n",
    "    start_loc = LOCAL_TZ.localize(start_dt)\n",
    "    end_loc = LOCAL_TZ.localize(end_dt)\n",
    "    \n",
    "    start_epoch = start_loc.timestamp()\n",
    "    end_epoch = end_loc.timestamp()\n",
    "\n",
    "    print(f\"Fetching DB data from {start_loc} to {end_loc}...\")\n",
    "\n",
    "    # 2. Query DB\n",
    "    with sqlite3.connect(DB_PATH) as conn:\n",
    "        query = \"\"\"\n",
    "        SELECT timestamp, event_type, parameter \n",
    "        FROM logs \n",
    "        WHERE timestamp >= ? AND timestamp < ?\n",
    "        ORDER BY timestamp\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(query, conn, params=(start_epoch, end_epoch))\n",
    "\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # 3. Process Columns\n",
    "    # Convert Epoch Float -> Datetime (UTC)\n",
    "    df['TS_start'] = pd.to_datetime(df['timestamp'], unit='s', utc=True)\n",
    "    \n",
    "    # Convert UTC -> US/Mountain\n",
    "    df['TS_start'] = df['TS_start'].dt.tz_convert(LOCAL_TZ)\n",
    "    \n",
    "    # Remove timezone info to match naive Pickles (if pickles are naive)\n",
    "    df['TS_start'] = df['TS_start'].dt.tz_localize(None)\n",
    "    \n",
    "    # Rename to match Old Format\n",
    "    df = df.rename(columns={'event_type': 'Code', 'parameter': 'ID'})\n",
    "    \n",
    "    # Drop the raw epoch column\n",
    "    return df[['TS_start', 'Code', 'ID']]\n",
    "\n",
    "def get_pickle_data(start_dt, end_dt):\n",
    "    \"\"\"\n",
    "    Finds relevant pickles by date and filters to exact time range.\n",
    "    \"\"\"\n",
    "    if not PICKLE_DIR.exists():\n",
    "        print(f\"❌ Pickle directory not found: {PICKLE_DIR}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 1. Find relevant files based on Date\n",
    "    # Pickles are named: df_raw_YYYY_MM_DD_HHMM-YYYY_MM_DD_HHMM.pkl\n",
    "    # We'll just grab files matching the dates in our range\n",
    "    \n",
    "    target_dates = pd.date_range(start_dt.date(), end_dt.date())\n",
    "    files_to_read = []\n",
    "    \n",
    "    for date in target_dates:\n",
    "        date_str = date.strftime('%Y_%m_%d')\n",
    "        # Glob for any file starting with this date\n",
    "        pattern = f\"df_raw_{date_str}_*.pkl\"\n",
    "        found = list(PICKLE_DIR.glob(pattern))\n",
    "        files_to_read.extend(found)\n",
    "    \n",
    "    if not files_to_read:\n",
    "        print(\"⚠️ No pickle files found for these dates.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Reading {len(files_to_read)} pickle file(s)...\")\n",
    "    \n",
    "    # 2. Load and Concatenate\n",
    "    dfs = []\n",
    "    for f in sorted(files_to_read):\n",
    "        try:\n",
    "            # Using bz2 as per your read_data.py\n",
    "            dfs.append(pd.read_pickle(f, compression='bz2'))\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading {f.name}: {e}\")\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    df_all = pd.concat(dfs)\n",
    "\n",
    "    # 3. Filter specific columns and time range\n",
    "    # Ensure columns exist (Pickles might have Cycle_start, Coord_plan, etc.)\n",
    "    cols = ['TS_start', 'Code', 'ID']\n",
    "    df_all = df_all[cols].copy()\n",
    "    \n",
    "    # Filter by time\n",
    "    mask = (df_all['TS_start'] >= start_dt) & (df_all['TS_start'] < end_dt)\n",
    "    return df_all[mask].sort_values(['TS_start', 'Code', 'ID']).reset_index(drop=True)\n",
    "\n",
    "def compare_datasets(start_time_str, end_time_str):\n",
    "    \"\"\"\n",
    "    Main Comparison Function.\n",
    "    Args:\n",
    "        start_time_str: 'YYYY-MM-DD HH:MM:SS' (24hr)\n",
    "        end_time_str:   'YYYY-MM-DD HH:MM:SS' (24hr)\n",
    "    \"\"\"\n",
    "    # Parse inputs\n",
    "    s_dt = datetime.strptime(start_time_str, '%Y-%m-%d %H:%M:%S')\n",
    "    e_dt = datetime.strptime(end_time_str, '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # --- GET DATA ---\n",
    "    print(\"--- Loading Data ---\")\n",
    "    df_new = get_db_data(s_dt, e_dt)\n",
    "    df_old = get_pickle_data(s_dt, e_dt)\n",
    "    \n",
    "    # --- ANALYSIS ---\n",
    "    print(\"\\n--- Comparison Results ---\")\n",
    "    print(f\"Time Window: {s_dt} to {e_dt}\")\n",
    "    print(f\"New DB Rows:   {len(df_new):,}\")\n",
    "    print(f\"Old Pickle Rows: {len(df_old):,}\")\n",
    "    \n",
    "    if df_new.empty and df_old.empty:\n",
    "        print(\"Both datasets are empty for this range.\")\n",
    "        return\n",
    "\n",
    "    # 1. Check for \"Break Records\" (Code 0)\n",
    "    # The new DB has these, the old pickles likely do not.\n",
    "    breaks_new = df_new[df_new['Code'] == 0]\n",
    "    real_data_new = df_new[df_new['Code'] != 0]\n",
    "    \n",
    "    print(f\"\\nNew DB Analysis:\")\n",
    "    print(f\"  - Real Data Rows: {len(real_data_new):,}\")\n",
    "    print(f\"  - Generated Breaks (Code 0): {len(breaks_new)}\")\n",
    "    \n",
    "    # 2. Direct Discrepancy (Count Mismatch)\n",
    "    diff = len(real_data_new) - len(df_old)\n",
    "    if diff == 0:\n",
    "        print(\"\\n✅ SUCCESS: 'Real' record counts match perfectly!\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ MISMATCH: Count difference of {diff} rows.\")\n",
    "\n",
    "    # 3. Visual Sampling\n",
    "    print(\"\\n--- Visual Sample (First 5 Rows) ---\")\n",
    "    print(\"NEW (DB):\")\n",
    "    display(df_new.head()) if 'display' in globals() else print(df_new.head())\n",
    "    print(\"OLD (Pickle):\")\n",
    "    display(df_old.head()) if 'display' in globals() else print(df_old.head())\n",
    "\n",
    "    print(\"\\n--- Visual Sample (Last 5 Rows) ---\")\n",
    "    print(\"NEW (DB):\")\n",
    "    display(df_new.tail()) if 'display' in globals() else print(df_new.tail())\n",
    "    print(\"OLD (Pickle):\")\n",
    "    display(df_old.tail()) if 'display' in globals() else print(df_old.tail())\n",
    "\n",
    "    return df_new, df_old\n",
    "\n",
    "# --- EXAMPLE USAGE ---\n",
    "# Run this cell, then uncomment the line below to test\n",
    "df1, df2 = compare_datasets('2025-11-25 17:00:00', '2025-11-28 22:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42695fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\rhansen\\\\Documents\\\\Python\\\\SPMs\\\\notebooks'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecf4bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9541cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading/decompressing FAILED_ECON_10.70.10.51_2025_01_23_0730.datZ: Error -3 while decompressing data: invalid stored block lengths\n"
     ]
    }
   ],
   "source": [
    "import zlib\n",
    "import struct\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def convert_datz_to_csv(input_file):\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Error: File {input_file} not found.\")\n",
    "        return\n",
    "\n",
    "    # 1. Decompress the file\n",
    "    try:\n",
    "        with open(input_file, 'rb') as f:\n",
    "            compressed_data = f.read()\n",
    "            # Decompress raw zlib data\n",
    "            full_content = zlib.decompress(compressed_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading/decompressing {input_file}: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Find the Text/Binary Boundary\n",
    "    # The header ends with the line \"Phases in use:...\" followed by a newline.\n",
    "    # We look for the newline character '\\n' (0x0A) after that specific marker.\n",
    "    marker = b\"Phases in use:\"\n",
    "    marker_pos = full_content.find(marker)\n",
    "    \n",
    "    if marker_pos == -1:\n",
    "        print(\"Error: Could not find header end marker.\")\n",
    "        return\n",
    "        \n",
    "    # Find the end of that line\n",
    "    newline_pos = full_content.find(b'\\n', marker_pos)\n",
    "    if newline_pos == -1:\n",
    "        print(\"Error: Header line not terminated.\")\n",
    "        return\n",
    "\n",
    "    # Split the data\n",
    "    header_bytes = full_content[:newline_pos+1]\n",
    "    binary_bytes = full_content[newline_pos+1:]\n",
    "\n",
    "    # 3. Parse the Header (to get the Base Timestamp)\n",
    "    header_text = header_bytes.decode('utf-8')\n",
    "    header_lines = header_text.splitlines()\n",
    "    \n",
    "    # We need the start time from the first line: \"11-25-2025 21:36:00.0,...\"\n",
    "    # Format: MM-DD-YYYY HH:MM:SS.f\n",
    "    first_line = header_lines[0]\n",
    "    date_part = first_line.split(',')[0] # Extract \"11-25-2025 21:36:00.0\"\n",
    "    \n",
    "    # Clean up format for Python parsing (replace - with / if needed, though strptime handles it)\n",
    "    # The binary header uses dashes, the CSV uses slashes.\n",
    "    try:\n",
    "        # Parse \"11-25-2025 21:36:00.0\"\n",
    "        # Note: Python < 3.7 might struggle with .0, so we handle microseconds manually if needed\n",
    "        time_str, micro_str = date_part.split('.')\n",
    "        base_time = datetime.strptime(time_str, \"%m-%d-%Y %H:%M:%S\")\n",
    "        base_time = base_time + timedelta(microseconds=int(micro_str) * 100000)\n",
    "    except ValueError:\n",
    "        print(f\"Error parsing date: {date_part}\")\n",
    "        return\n",
    "\n",
    "    # 4. Parse the Binary Data\n",
    "    # Schema found: 4 Bytes per row\n",
    "    # Byte 0: Event Type (Integer)\n",
    "    # Byte 1: Parameter (Integer)\n",
    "    # Byte 2-3: Time Offset (Unsigned Short, Big Endian) -> tenths of a second\n",
    "    \n",
    "    row_size = 4\n",
    "    num_rows = len(binary_bytes) // row_size\n",
    "    \n",
    "    csv_rows = []\n",
    "    \n",
    "    # Add Header (Matches your CSV format)\n",
    "    csv_rows.append(\"Timestamp,Event Type,Parameter\")\n",
    "    \n",
    "    # (Optional) Reconstruct the Metadata rows from the header_lines if you need them in the CSV\n",
    "    # Your example CSV includes metadata rows at the top. \n",
    "    # For now, let's process the binary data which is the critical part.\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        chunk = binary_bytes[i*row_size : (i+1)*row_size]\n",
    "        \n",
    "        # Unpack: > = Big Endian, B = uchar (1 byte), H = ushort (2 bytes)\n",
    "        event_type, parameter, time_offset = struct.unpack('>BBH', chunk)\n",
    "        \n",
    "        # Calculate actual time\n",
    "        # Offset is in 10ths of a second\n",
    "        row_time = base_time + timedelta(seconds=time_offset/10.0)\n",
    "        \n",
    "        # Format Timestamp: MM/DD/YYYY HH:MM:SS.f (1 digit precision)\n",
    "        ts_str = row_time.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "        ms_digit = row_time.microsecond // 100000\n",
    "        final_ts = f\"{ts_str}.{ms_digit}\"\n",
    "        \n",
    "        csv_rows.append(f\"{final_ts},{event_type},{parameter}\")\n",
    "\n",
    "    # 5. Save to CSV\n",
    "    output_filename = input_file.replace(\".datZ\", \".csv\")\n",
    "    with open(output_filename, 'w', newline='') as f:\n",
    "        f.write('\\n'.join(csv_rows))\n",
    "\n",
    "    print(f\"Success! Converted {input_file} -> {output_filename}\")\n",
    "    print(f\"Processed {num_rows} binary records.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
